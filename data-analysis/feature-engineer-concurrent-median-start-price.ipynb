{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "pd.set_option('display.max_columns', 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Helper Fuctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "\n",
    "def clean_text(doc, remove_stop_words=True, remove_digits=False, remove_punc=True, stem=False):\n",
    "    \n",
    "    # 1. Remove any HTML markup\n",
    "    text = BeautifulSoup(doc).get_text()  \n",
    "    \n",
    "    # 2. Extract special negator like n't\n",
    "    text = re.sub('n\\'t', ' not', text)\n",
    "    \n",
    "    # 3. remove punctuation(except .-)\n",
    "    if remove_punc:\n",
    "        text = re.sub('[^a-zA-Z.\\-\\d]', ' ', text)\n",
    "        \n",
    "    if remove_digits:\n",
    "        text = re.sub('[.\\d]', ' ', text)\n",
    "        \n",
    "    # 4. Convert to lower case \n",
    "    text = text.lower()\n",
    "        \n",
    "    # 5. Remove stop words\n",
    "    if remove_stop_words:\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text.split(' ') if not w in stops]\n",
    "        text = ' '.join(text)\n",
    "                \n",
    "    # 6. apply Porter Stemming\n",
    "    # probably don't need this\n",
    "    if stem:\n",
    "        stemmer = PorterStemmer()\n",
    "        stemmer = LancasterStemmer()\n",
    "        text = [stemmer.stem(w) for w in text.split(' ')]\n",
    "        text = ' '.join(text)\n",
    "        \n",
    "    # 7. Remove extra white space\n",
    "    text = re.sub(' +',' ', text)\n",
    "        \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auctions = pd.read_pickle('./pickles/auctions.p')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select features from which similarity will be calcualted  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Select Features from which similarity will be calcualted \n",
    "title_series = auctions['title']\n",
    "condition_id_series = auctions['condition.conditionId']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take start/end times and start price features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Necessary Features\n",
    "start_price_series = auctions['startPrice']\n",
    "start_time_series = auctions['listingInfo.startTime']\n",
    "end_time_series = auctions['listingInfo.endTime']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean Text Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning #5000 out of 29961 documents\n",
      "cleaning #10000 out of 29961 documents\n",
      "cleaning #15000 out of 29961 documents\n",
      "cleaning #20000 out of 29961 documents\n",
      "cleaning #25000 out of 29961 documents\n"
     ]
    }
   ],
   "source": [
    "clean_titles = []\n",
    "for i,title in enumerate(title_series.values):\n",
    "    if (i+1)%5000==0:\n",
    "        print 'cleaning #{} out of {} documents'.format(i+1,len(condition_combined))\n",
    "    clean_titles.append(clean_text(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cleaning #5000 out of 29961 documents\n",
      "cleaning #10000 out of 29961 documents\n",
      "cleaning #15000 out of 29961 documents\n",
      "cleaning #20000 out of 29961 documents\n",
      "cleaning #25000 out of 29961 documents\n"
     ]
    }
   ],
   "source": [
    "clean_conditions = []\n",
    "for i,cond in enumerate(condition_combined):\n",
    "    if (i+1)%5000==0:\n",
    "        print 'cleaning #{} out of {} documents'.format(i+1,len(condition_combined))\n",
    "    clean_conditions.append(clean_text(cond))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Vectorize text features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range = (1,2),\n",
    "                             min_df=10,\n",
    "                             analyzer='word',\n",
    "                             stop_words=None,\n",
    "                             max_features=10000,\n",
    "                            )\n",
    "\n",
    "titles_matrix = vectorizer.fit_transform(clean_titles)\n",
    "titles_df = pd.DataFrame(titles_matrix.todense(), columns=vectorizer.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "most common condition grams: [(u'zoom', 772), (u'years', 771), (u'year', 770), (u'wrong', 769), (u'wrist strap', 768)]\n",
      "least common condition grams: [(u'000', 0), (u'10', 1), (u'100', 2), (u'12', 3), (u'14', 4)]\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range = (1,2),\n",
    "                             min_df=30,\n",
    "                             analyzer='word',\n",
    "                             stop_words=None,\n",
    "                             max_features=5000,\n",
    "                            )\n",
    "\n",
    "conditions_matrix = vectorizer.fit_transform(clean_conditions)\n",
    "conditions_df = pd.DataFrame(conditions_matrix.todense(), columns=vectorizer.vocabulary_.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del clean_titles\n",
    "del clean_conditions\n",
    "del conditions_matrix\n",
    "del titles_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create new dataframe from pre-processed features**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# data_frames_to_keep = [titles_df, conditions_df, start_time_series, end_time_series, start_price_series]\n",
    "data_frames_to_keep = [titles_df, condition_id_series, start_time_series, end_time_series, start_price_series]\n",
    "auctions = pd.concat(data_frames_to_keep, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# del title_series\n",
    "# del condition_series\n",
    "# del start_price_series\n",
    "# del start_time_series\n",
    "# del end_time_series"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Preprocessing imported dataframe**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create medianConcurrentStartPrice column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "auctions['medianConcurrentStartPrice'] = np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sort dataframe by startTime DESC\n",
    "auctions.sort_values(by='listingInfo.startTime', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "auctions.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Find Concurrent Listings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set threshold for similarity, as well as minimum number of similar items\n",
    "top_n_items = 5\n",
    "threshold = 0.999999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Sample listing \n",
    "sample_index = 1000\n",
    "current_listing = auctions.iloc[sample_index] \n",
    "listing_st = auctions['listingInfo.startTime'][sample_index] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# subset dataframe\n",
    "auctions_subset = auctions.iloc[:sample_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find concurrent listings\n",
    "concurrent_listings_df = auctions_subset[auctions_subset.apply(lambda x: listing_st<x['listingInfo.endTime'], axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the similarity between current listing \n",
    "current_listing_vec = current_listing.iloc[1:-3].values.reshape(1,-1)\n",
    "concurrent_listings_matrix = concurrent_listings_df.iloc[1:, :-3].values\n",
    "cos_sim_matrix = cosine_similarity(current_listing_vec, concurrent_listings_matrix)\n",
    "concurrent_listings_df.insert(loc=concurrent_listings_df.shape[1]-1, column='similarity_score', value=cos_sim_matrix.reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find top n most similar items\n",
    "maxSimScore = max(concurrent_listings_df['similarity_score'])\n",
    "for i,threshold in enumerate(np.arange(1, 0.9999000, -0.0000005)):\n",
    "    # Filter for similar items\n",
    "    numSimilarListings = concurrent_listings_df[concurrent_listings_df['similarity_score']>maxSimScore*threshold].shape[0]\n",
    "    if numSimilarListings >= top_n_items:        \n",
    "        concurrent_similar_listings_df = concurrent_listings_df[concurrent_listings_df['similarity_score']>maxSimScore*threshold]\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Find median start price \n",
    "medianStartPrice = np.median(concurrent_similar_listings_df['startPrice'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set median to dataframe\n",
    "# auctions.loc[sample_index,'medianConcurrentStartPrice'] = medianStartPrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def filter_fn(listing):\n",
    "    top_n_items = 5\n",
    "    threshold = 0.90\n",
    "\n",
    "    current_listing = listing\n",
    "    current_listing_index = listing['index']\n",
    "    listing_st = current_listing['listingInfo.startTime']\n",
    "    \n",
    "    # subset dataframe\n",
    "    auctions_subset = auctions.iloc[:current_listing_index]\n",
    "\n",
    "    # Find concurrent listings\n",
    "    concurrent_listings_df = auctions_subset[auctions_subset.apply(lambda x: listing_st<x['listingInfo.endTime'], axis=1)]\n",
    "\n",
    "    # Calculate the similarity between current listing \n",
    "    current_listing_vec = current_listing.iloc[:-4].values.reshape(1,-1)\n",
    "    concurrent_listings_matrix = concurrent_listings_df.iloc[:, :-4].values\n",
    "    \n",
    "#     print current_listing_vec.shape\n",
    "#     print concurrent_listings_matrix.shape\n",
    "    \n",
    "    cos_sim_matrix = cosine_similarity(current_listing_vec, concurrent_listings_matrix)\n",
    "    \n",
    "#     print cos_sim_matrix.reshape(-1,1).shape\n",
    "#     print concurrent_listings_df.shape\n",
    "    \n",
    "    concurrent_listings_df.insert(loc=concurrent_listings_df.shape[1]-1, column='similarity_score', value=cos_sim_matrix.reshape(-1,1))\n",
    "\n",
    "    # Find top n most similar items\n",
    "    maxSimScore = max(concurrent_listings_df['similarity_score'])\n",
    "    for i,threshold in enumerate(np.arange(1, threshold, -0.05)):\n",
    "        # Filter for similar items\n",
    "        numSimilarListings = concurrent_listings_df[concurrent_listings_df['similarity_score']>maxSimScore*threshold].shape[0]\n",
    "        if numSimilarListings >= top_n_items:  \n",
    "            concurrent_similar_listings_df = concurrent_listings_df[concurrent_listings_df['similarity_score']>maxSimScore*threshold]\n",
    "            break\n",
    "        \n",
    "        \n",
    "    try:\n",
    "        medianStartPrice = np.median(concurrent_similar_listings_df['startPrice'])\n",
    "    except:\n",
    "        medianStartPrice = np.nan\n",
    "    return medianStartPrice\n",
    "                             \n",
    "                   \n",
    "\n",
    "auctions.iloc[1000:6000].apply(filter_fn, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Vrushank's Code\n",
    "#     min_overlap = pd.to_timedelta('{} hour'.format(hours_overlap))\n",
    "#     overlapping_listings = auctions.apply(lambda comp_list: \n",
    "#                                           min(comp_list['listingInfo.endTime'], listing['listingInfo.endTime']) - \\\n",
    "#                                           max(comp_list['listingInfo.startTime'], listing['listingInfo.startTime']) > \\\n",
    "#                                           min_overlap, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
