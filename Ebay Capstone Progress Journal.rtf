{\rtf1\ansi\ansicpg1252\cocoartf1504\cocoasubrtf810
{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fmodern\fcharset0 Courier;\f2\froman\fcharset0 Palatino-Roman;
\f3\fmodern\fcharset0 Courier-Bold;\f4\fswiss\fcharset0 ArialMT;\f5\fnil\fcharset0 HelveticaNeue;
}
{\colortbl;\red255\green255\blue255;\red0\green0\blue0;\red85\green142\blue40;\red0\green0\blue0;
\red255\green83\blue8;\red133\green0\blue175;\red174\green0\blue240;\red255\green255\blue255;\red255\green39\blue18;
\red63\green105\blue30;\red255\green255\blue51;\red179\green179\blue179;\red128\green128\blue128;\red255\green250\blue131;
\red38\green38\blue38;\red255\green255\blue255;\red194\green229\blue166;\red192\green237\blue254;\red255\green252\blue171;
\red255\green164\blue159;\red254\green187\blue100;\red194\green229\blue166;\red0\green0\blue0;\red255\green255\blue255;
}
{\*\expandedcolortbl;;\cssrgb\c0\c0\c0;\csgenericrgb\c33333\c55686\c15686;\csgenericrgb\c0\c0\c0;
\csgenericrgb\c100000\c32549\c3137;\csgenericrgb\c52157\c0\c68627;\csgenericrgb\c68235\c0\c94118;\csgenericrgb\c100000\c100000\c100000;\csgenericrgb\c100000\c15294\c7059;
\csgenericrgb\c24706\c41176\c11765;\csgenericrgb\c100000\c100000\c20000;\csgray\c75407;\csgray\c57415;\csgenericrgb\c100000\c98039\c51373;
\cssrgb\c20000\c20000\c20000;\cssrgb\c100000\c100000\c100000;\csgenericrgb\c76078\c89804\c65098;\csgenericrgb\c75294\c92941\c99608;\csgenericrgb\c100000\c98824\c67059;
\csgenericrgb\c100000\c64314\c62353;\csgenericrgb\c99608\c73333\c39216;\csgenericrgb\c76078\c89804\c65098;\cssrgb\c0\c0\c0;\cssrgb\c100000\c100000\c100000;
}
\margl1440\margr1440\vieww19000\viewh9060\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 eBay Capstone Work Journal:\
I chose cameras because:\
\pard\pardeftab720\partightenfactor0
\cf2 \expnd0\expndtw0\kerning0
a) they can be evaluated by using well-structured data that lends itself well to machine learning techniques\
b) they are representable as a set of easily quantified parameters\
c) they represent a large market of used items\
d) their prices are predictable and relatively stable for short-term horizons. \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \kerning1\expnd0\expndtw0 \
\
3/21 - 3/27:\
- Experimented with MongoDB, didn\'92t like fact that data was stored in JSON, and the fact that some rows can have different columns.\
- Chose postgresql due to familiarity with SQL, flexible types, \
- setup postresql ebay database with completed_items table under user: nathan\
- Set up ebay API to get completed items and store data into postgres database. \
- set up multi-processed web scraper with scrapy and multiprocessing.Pool to update \'93condition\'94 fields in ebay table from scraped item condition data. Sometimes the item doesn\'92t have a condition, in which case, an empty string is put into the field. \
\
3/28:\
Goal:\
in \
/Users/Naekid/Desktop/capstone-DSI-5/ebay-price-predictor/ebay-api-scraper/ebay_scraper/ebay_scraper \
rub scrapy\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf3 $ scrapy crawl ebay_crawl_spider -a url_start_index=92224\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf4 enter postgres database \cf3 \
$ psql - U nathan ebay
\f0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
- Setup up Scrapy spider. \
	- Following tutorial, when I ran scrapy crawl ebay, I got TypeError: \'91float\'92 object is not itterable. \
		- Solved problem by updating scrapy using conda. Only problem is that I also updated scrapy using conda system-wide, which scrapy documentation told me not to do (I updated before reading that). \
	- Creating pipeline.\
		- created scrapy->postgres pipeline class for storing condition data into the ebay table \
		- added pipeline to settings.py\
		- Need to read itemId,URL from ebay table (x)\
		- Need to set options for throttling (x)\
		- Set postgres config in settings.py (x)\
	- Test spider with hardcoded ebay urls\
		- Worked but it\'92s slow (x)\
		- Speed up scrapy crawl ebay  (X)\
		- Create log of errors from scraps (X)\
\
3/29:\
- Pull postgres data into pandas data frame using sqlalchemy (x)\
	- Deleted duplicates in completed_items table (x)\
	
\f1 \cf3 DELETE FROM tablename\
	WHERE id IN \
		(SELECT id FROM (SELECT id, ROW_NUMBER() OVER (partition BY column1, column2, column3 ORDER BY id) AS rnum FROM tablename) \
		as t WHERE t.rnum > 1);\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf4 or specifically for deleting duplicate itemId :\cf3 \
\
DELETE FROM completed_items_v2 WHERE id IN (SELECT id FROM (SELECT ci."id", ROW_NUMBER() OVER (partition BY ci."itemId" ORDER BY ci."id") AS rnum FROM completed_items_v2 as ci) as t WHERE t.rnum > 1);\cf5 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf0 \
\
- Light cleaning (x)\
- Light EDA (x)\
- Discovered that I had a typo in my find-completed-listings.py, which was causing the \'93categoryId\'94 search in my filter to not work.\
Therefore I was getting completed items from every ebay category (only filter I was using was the keyword \'91camera\'92). \
	- apiy_request dictionary had \'91CategoryId\'94 which should have been \'93categoryId\'94. \
	- Not sure what to do now. I can change the categoryId to the right category and then start my api requests from the max  But then I will get a lot of duplicate items. \
	- I think the best strategy is to just copy my old postgres table into a new table so I don\'92t lose my searches thus far. \
	- Then filter that table by the categoryId in SQL. \
	- Then begin filling that table with new data starting at a low price, using no keywords. \
	\
- Vectorize title with CountVectorizer (x) \
- Use NLP features + model to predict sold status (x)\
	- Baseline accuracy = 0.897\
	- First pass model (random forest) accuracy = 0.913\
\
3/30:\
- Create new table\
	- Copy completed_items (x)\
	- Filter: (x)\
		- categoryId = 15230 # film cameras\
		- categoryId = 31388 # digital cameras \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf0 		\cf3 DELETE FROM completed_items_15230_31388 as ci WHERE ci."primaryCategory.categoryId"!=31388 AND ci."primaryCategory.categoryId"!=15230;
\f0 \cf0 \
	- OK that didn\'92t work. When we copy completed_items, a serial (id_seq table) was not created. Therefore, in our completed_items_15230_31388 table, the id column was not a serial, and was not being automatically generated when we inserted values. \
		- I copied old table to new table. Then I created a new serial primary key column. Then I deleted the old id column. Then I deleted duplicate rows (rows with duplicate itemId values). \
\
\
- using categoryId = 15230, 31388 and no keywords, query starting at minPrice = 20, up until 2500 (x)\
	- For some reason, I was able to query as much data using the ebay API with only one dev API key. Not sure why. \
\
\
Goals for 3/31 and weekend:\
- Use scrapy to do recursive scrap and get starting bid price.\
- Continue NLP exploration and modeling with different features and techniques.\
\
3/31:\
- What happens to values returned or yielded from parse() and parse_start_prce() in spider?\
- Where to instantiate Item()? \
- How to update item class instance in spider within different callback functions? \
\
4/1\
- Got nested scrape working, but now ebay is requesting captcha every time I want to scrape. \
- Using cactusVPN, I was able to start scraping again. \
	- VPN disconnects if I send too many requests too fast. Using these settings, it seems to work:\
		
\f1 \cf6 AUTOTHROTTLE_START_DELAY = 0.5\
		AUTOTHROTTLE_MAX_DELAY = 2\
		AUTOTHROTTLE_TARGET_CONCURRENCY = 2\
		CONCURRENT_REQUESTS = 2\
		DOWNLOAD_DELAY = 0.9
\f0 \cf0 \
- So I\'92m collecting itemId, conditiondescription, duration, and startPrice now. \
\
\
I was thinking about what factors we\'92re going to consider in the model. I think that, along with completed listings, a big factor would be the other listings that are active at the time the model does it\'92s calculation for a specific item. \
\
The reason we want to see which items were listed concurrently is because (I would think) you would want to weight the factors from the concurrent listings more than the previously completed listings. Like if you are selling a Nikon ec380, and there is already a Nikon ec380 listed with a startPrice of $300, and all factors being equal, you probably don\'92t want to list it for much more than $300. Even if historically those cameras sell with startPrice of $400, you probably don\'92t want to list it for $400. Therefore I want to weight the concurrent listings factors more than the completed listings. \
\
Is it possible to do something like this? Is it even smart to do something like this? \
\
Also, I don\'92t see anyone doing this in the white papers that have been released previously for ebay listings end price predictions. Could be a novel method. \
\
\
(You could figure out which items are concurrent by looking at the startTime and endTime columns. You could then create a new column called concurrentGroupNumber, that would be generated by incrementing through the first startTime to the last startTime in equally spaced chunks. In each chunk, you look at all the listings that are active in that chunk, and for each of those listings, assign the same groupNumber to them. That allows you to see concurrent listings.)\
\
Goals for 4/1 and 4/2:\
- Scrape more data. Current number of rows in 
\f1 \cf7 completed_items_15230_31388 is 46225. 
\f0 \cf0 \
	- Collect completed_items auctions for starting at 3/30 at 11:10:00, until 4/2 12:00:00 (x)\
	- collect buy it now auctions of prices from 20 to 2500 and store in table completed_items_15230_31388 (x) \
		- 
\f1 \cf7 \{'name': 'ListingType', 'value':'AuctionWithBIN'\},
\f0 \cf0 \
	- Collect fixedPrice auctions of prices from 20 to 2500 and store in table completed_items_15230_31388 \
- Don\'92t know if \'91FixedPrice\'92 listings were sold or if they just ended. \
	- Okay this is a big problem. It seems like \'91AuctionWithBIN\'92 is like a deprecated feature because if you go on ebay.com now, you can only find auctions OR BuyItNow listings, but there don\'92t seem to be listings that are AuctionWithBIN. \
	- So I might have to just scrape data for the Buy It Now listings. \
		- Although the only problem I\'92m seeing here is that you can\'92t find Buy It Now listings from stores\'85 which is a problem because stores make up a lot of the business.\
\
- I experimented with changing the code of connections in:\

\f1  /Users/Naekid/anaconda3/envs/dsi/lib/python2.7/site-packages/ebaysdk/connection.py
\f0 \
and then running the samples in:\

\f1 /Users/Naekid/anaconda3/envs/dsi/lib/python2.7/site-packages/ebaysdk-2.1.4-py2.7.egg/samples\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf0 For some reason, the samples in .egg use the code in the other ebaysdk/ directory. Who knows. \
\
I created a GitHub ticket on the python SDK page. I also created a ebay developers forum post. \
Here\'92s a sample URI HTTP GET with the correct sellingState:\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
{\field{\*\fldinst{HYPERLINK "http://svcs.ebay.com/services/search/FindingService/v1?OPERATION-NAME=findCompletedItems&SERVICE-VERSION=1.7.0&SECURITY-APPNAME=nathanzo-ebaypric-PRD-cbed4d450-05d217d8&RESPONSE-DATA-FORMAT=XML&keywords=222461424089"}}{\fldrslt \cf0 http://svcs.ebay.com/services/search/FindingService/v1?OPERATION-NAME=findCompletedItems&SERVICE-VERSION=1.7.0&SECURITY-APPNAME=nathanzo-ebaypric-PRD-cbed4d450-05d217d8&RESPONSE-DATA-FORMAT=XML&keywords=222461424089}}\
s\
I\'92ve decided that the best course of action is to move on to sending my own HTTP GET requests to ebay using the structure outlined in their API documentation. \
\
4/3:\
- Discovered bug that was causing incorrect sellingState to be returned. The problem was that the the 
\f1 X-EBAY-SOA-SERVICE-VERSION 
\f0 Header field in the ebaySDK was set to 1.0.0. When I hardcoded it to 1.13.0, that fixed the problem. Also, you should be able to change the version in the ebay.yaml file, but that didn\'92t work for me.\
\
- Wrote my own wrapper for Ebay API to send findCompletedItems requests to specific itemId, retreive sellingState, and update SQL table. \
\
- scrape ebay for condition information of FixedPrice items. \
\
- my-ebay-api-port/development.ipynb is for using HTTP GET ebay API to update sellingState for items that are already in my ebay database. \
- scrapy is used to get condition description for all items, and startPrice for items that had > 0 bid_count.\
	- scrapy is not getting startPrice successfully. \
\
\
4/4:\
- I\'92m thinking of starting a new database from scratch since the old one is super messy. \
	- First objective is to get the startPrice.\
	- Once I have the startPrice, then I can use find-completed-items on \'91Auction\'92,\'92AuctionWithBIN\'92,\'92FixedPrice\'92,\'92StoreInventory\'92\
\
- Making new spider: ebay_spider_v2\
	- All I need to scrape is the conditionDesription, startPrice \
- Turns out the two different bid history pages are due to 2 different version of the page. For example:\
	{\field{\*\fldinst{HYPERLINK "http://www.ebay.com/bfl/viewbids/291989472205?item=291989472205&rt=nc&_trksid=p2047675.l2565"}}{\fldrslt http://www.ebay.com/bfl/viewbids/291989472205?item=291989472205&rt=nc&_trksid=p2047675.l2565}}\
	{\field{\*\fldinst{HYPERLINK "http://offer.ebay.com/ws/eBayISAPI.dll?ViewBids&item=291989472205&rt=nc&_trksid=p2047675.l2565"}}{\fldrslt http://offer.ebay.com/ws/eBayISAPI.dll?ViewBids&item=291989472205&rt=nc&_trksid=p2047675.l2565}}\
Slightly different - I don\'92t know how I\'92m getting these two. But it\'92s not a big deal. We can just grab the entire HTML of either page, and use this code to get the startPrice:\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf3 bid_history_items = response.xpath("//span/text()").extract()\
if bid_history_items:\
	for i,text in enumerate(bid_history_items):\
		if text == 'Starting Price':\
			startPrice = bid_history_items[i+1]\
			item['startPrice'] = float(startPrice.replace('$',''))\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf0 It seems to work. \
\
- I also found out that sometimes if a listing is an auction but ends with a BuyItNow, the findCompletedItems does not always show that it was a \'91AuctionWithBIN\'92 or that it ended with a BuyItNow. It\'92s also going to require two nested scrapes to get the start price. \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf8 \cb9 I will have to investigate this further, but looking into listings of type \'91AuctionWithBIN\'92\
\cf4 \cb1 And seeing if they are fucked up.\cf0 \
\
\
4/5:\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf10 - I think that a unique contribution to this problem would be to utilize concurrent listings, as well as completed listings.\

\fs28 	- 
\fs24 Calculate 
\i similar
\i0  (via NLP), concurrent (defined below) listing mean/median starting price. This alone is a useful application of ML.\
	- Calculate similar completed listing mean/median start price. \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf11 \cb12 It might be a good idea to consider the price of similar listings 
\i at the time of posting
\i0  a given listing, but that would require lots of extra scraping. \cf4 \cb1 \
\cf11 \cb13 So instead, we\'92ll just consider the 
\i start price 
\i0 of concurrent listings.\cf0 \cb1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\fs28 \cf10 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\fs24 \cf0 \
FEATURE ENGINEERING for mean/median startPrice of Concurrent Listings, and mean/median of startPrice and endPrice of Completed Listings. \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf0 FIRST create 4 new columns in dataframe \
	similarConcurrentListing.meanStartPrice\
	similarConcurrentListing.medianStartPrice\
	similarCompletedListing.meanStartPrice\
	similarCompletedListing.medianStartPrice\
	similarCompletedListing.meanEndPrice\
	similarCompletedListing.meanEndPrice\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf0 \
We need to define concurrent listings.
\f1 \
\

\f2 Filter for Concurrent and Completed Listings
\f1 \
1 Method - \
	if listing2.starTime > listing1.startTime, then l2.startTime should be < H1 hours after l1.starTime \
	If listing2.startTime < listing1.startTime, then l2.endTime should be > H2 hours after listing1.startTime\
We could just choose H1 and H2, but maybe EDA is a good way of finding this best time. We could also use H1 and H2 as hyper parameters.\
\
sort dataframe by startTime DESC \
for listing1 in listings:\
	for listing2 in listing: # only consider concurrent and past listings (exclude future listings)\
		if listing2.startTime < listing1.endTime and listing2.endTime > listing1.startTime:\
			# listing1 and listing2 are concurrent listings\
			add to dataframe with [\'91concurrent\'92]=1		\
				\
		elif listing2.endTime <= listing1.startTime:\
			# listing2 is a past listing for listing1 \
			add to data frame with [\'91concurrent\'92]=0\
\
\
2 Method - \
Actually instead, let\'92s frame the problem so that we calculate the percentage of overlap between the time periods of l1 and l2. Define l2 to be a concurrent listing if it\'92s percentage overlap with l1 is > H, where H is a number between 0 and 1. H could be a hyper parameter to the modeling pipeline. \
\
sort dataframe by startTime DESC \
for listing1 in listings:\
	for listing2 in listing: # only consider concurrent and past listings (exclude future listings)\
		if listing2.startTime < listing1.endTime and listing2.endTime > listing1.startTime:\
			# listing1 and listing2 are concurrent listings\
			add to dataframe with [\'91concurrent\'92]=1		\
				\
		elif listing2.endTime <= listing1.startTime:\
			# listing2 is a past listing for listing1 \
			add to data frame with [\'91concurrent\'92]=0\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f3\b \cf0 3 Method - THE BEST (most realistic)
\f1\b0 \
sort dataframe by startTime DESC\
for listing1 in listings:\
	create new dataframe for concurrent and completed listings (no future listings)\
	add listing1 to new dataframe\
	for listing2 in listings:\
		if listing2.startTime < listing1.startTime and listing2.endTime > listing1.startTime:\
			add listing2 to new dataframe with [\'91concurrent\'92]=1\
		else:\
			add listing2 to new dataframe with [\'91concurrent\'92]=0\
	calculate median startPrice of listings with [\'91concurrent\'92]==1
\f2 \
\
For EDA on that aspect, Vrushank suggested plotting percentage of concurrent time (x-axis) vs. number of listings within that time, although that wouldn\'92t tell you how much the hyper parameter affects the accuracy of the model.
\f1 \
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf0 VECTORIZE
\f1 \
	Vectorize new dataframe (BoW, TFIDF, shingles, sPacy) as vdf\
	Calculate similarity (Jaccard, Cosine), between listing1 and all other listings in vdf\
	Set a threshold for similairty so that only items OVER that threshold will be considered in the model, i.e. filter vdf so that we only are considering listings very similar to the original listing\
	# Now you have a vectorized df of listings similar to listing1\
	calculate mean/median of concurrent=1 listings and concurrent=0 listings, seperately\
	You now have 2 new features for your listing, which you can add as new columns to the row for listing1 in the original dataframe\
	\
\
\

\f2 Just had an idea. Essentially, instead of finding similar titles myself, I use ebay\'92s search to find similar titles using topics created by LDA to find the commonly searched keywords from the titles that I already have in my database. Then, for each LDA topic/keyword, you search bay\'92s current and completed listings. But then you still have to calculate similarity, because your alternative is to just take the top n searches from ebay\'92s results, and obviously some of those results will be very different to each other. But using ebay\'92s search results combined with my own similarity calculation might be more effective than JUST using my own similarity calculation. Let\'92s try using my own similarity calculation first because we will have to do that anyway. 
\f1 \
\
		\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf4 \cb14 Problem: People can list items as auctions, and update those listings midway through to have BuyItNow available, but this won\'92t show up in a grab from the ebay API. \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \cb1 From the documentation:\
\pard\pardeftab720\sl300\partightenfactor0

\f4\i\fs26 \cf15 \cb16 \expnd0\expndtw0\kerning0
Auction\
Competitive-bid online auction format. Buyers engage in competitive bidding, 
\b although Buy It Now may be offered as long as no valid bids have been placed
\b0 .\

\i0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2\fs24 \cf0 \cb1 \kerning1\expnd0\expndtw0 This is happening for certain listings, such as 332172404108. \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b \cf0 It has a listingType of \'91Auction\'92, no \'91startPrice\'92, BuyItNowAvailable=\'91f\'92, and bidCount=1, then that final selling bid was a Buy It Now.
\b0  \
This must have happened because the user updated their listing after it was already posted and online for some time. \
\
xpath for \'91see original listing\'92 for these pages:\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf10 //span[contains(@class, 'vi-inl-lnk')]//@href\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
There are two options here.\
1. We could go and scrape for the start price, and then make two listings out of each of these listings: 1 would be an \'91Auction\'92 listing that did NOT sell, and had the original start price. And the other would be a \'91AuctionWithBIN\'92 that DID sell. \
2. We just make the listing a \'91AuctionWithBIN\'92 that DID sell, with a start price equal to the sale price.\
\
Let\'92s see how many of these listings there are. if there are a lot, that gives more reason to do option 1, because if we don\'92t, we are losing out on significant information with the \'91Auction\'92 listings that did not sell. But it\'92s going to take a lot of rescraping to get those, and just adds a shit ton of complications.\
The number of listings (so far on 4/5) is 4,432. \
I mean it\'92s not a big deal to go with option 2, because we\'92re just kind of pretending those \'91Auction\'92 listings didn\'92t exist, which isn\'92t horrible.\
\
So to deal with these listings with option 2:\
# listingType of \'91Auction\'92, no \'91startPrice\'92, BuyItNowAvailable=\'91f\'92, and bidCount=1\
mask = (df['listingInfo.listingType']=='Auction')\\\
        & (np.isnan(df['startprice'])) \\\
        & (df['sellingStatus.bidCount']==1.0)\
dfm = df[mask]\
dfm[\'91listingInfo.listingType\'92] = \'91AuctionWithBIN\'92\
dfm[\'91listingInfo.listingType\'92] = \'91AuctionWithBIN\'92\
\
OKAY and this new information:\
\pard\pardeftab720\sl300\partightenfactor0

\f4\i\fs26 \cf15 \cb16 \expnd0\expndtw0\kerning0
AuctionWithBIN\
Same as Auction format, but Buy It Now is enabled. AuctionWithBIN changes to Auction if a valid bid has been placed on the item.
\i0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2\fs24 \cf0 \cb1 \kerning1\expnd0\expndtw0 Which means we have funky information BUT here is what I realized. Because ebay doesn\'92t update \'91Auction\'92 to \'91AuctionWithBIN\'92 for listings where the user updated the listing type midway through\'85\
\
\'85We need to create our own 
\b endListingType
\b0 , that concerns the end state of the listing. \
IF an item did NOT SELL, then we care about the END state (that\'92s the only state there is). \
	- if sellingState = \'91EndedWithoutSales\'92:\
		- 
\b endListingType 
\b0 = listingInfo.listingType\
		- 
\b startprice 
\b0 = sellingStatus.currentPrice.value\
IF an item SOLD, then we care how about the END state (the state of the item when it was sold)\
	- else if sellingState = \'91EndedWithSales\'92:\
		- If listingInfo.listingType was \'91Auction\'92:\
			- 
\b if listingType= \'91Auction\'92,  \'91startprice\'92=NaN,  \'92bidCount\'92=1.0 (because user can only change to BIN w/ 0 bids), then that final selling bid was a Buy It Now.
\b0  This also catches the case where an AuctionWithBIN \
				- 
\b endListingType
\b0  = \'91AuctionWithBIN\'92\
				- 
\b startprice
\b0  = sellingStatus.currentPrice.value\
			- 
\b if listingType= \'91Auction\'92,  \'91startprice\'92 
\fs32 !=
\fs24  NaN\
				- endListingType 
\b0 = \'91Auction\'92\
				- 
\b startprice
\b0  = startprice\
		- if listingInfo.listingType = \'91AuctionWithBIN\'92:  # see note\
			
\b - endListingType 
\b0 = \'91AuctionWithBIN\'92\
			- 
\b startprice
\b0  = sellingStatus.currentPrice.value\
# I believe this is only true for items that stayed as \'91AuctionWithBIN\'92 the entire life of the listing, and if I\'92m right\'85\
# I\'92m looking for an \'91AuctionWithBIN\'92 that changed to an \'91Auction\'92 because someone put a bid, but then eventually it SOLD as BuyItNow.\
# I don\'92t that exists because (from documentation): 
\f4\i On most sites, 
\b the Buy It Now option is removed (and this value returns false) once a valid bid is made on the associated item
\b0  (a valid bid could be a bid above the reserve price). buyItNowAvailable will return "false" if the listing type is anything but "AuctionWithBIN". Please ignore buyItNowAvailable for fixed-price listings.
\i0 \

\f2 \
	- if listingInfo.listingType= \'91FixedPrice\'92 or listingInfo.listingType= \'92StoreInventory\'92: # \'91FixedPrice\'92 means PURE BuyItNow, and no Auction available. \
		-
\b  endListingType 
\b0 = \'91FixedPrice\'92 or \'91StoreInventory\'92\
		- start price = sellingStatus.currentPrice.value 
\f0 \
\
\
\
\
\

\f2 4/6:\
Created baseline model. Used the following features:\
[titles_df, conditions_df, auction_condition_dummies, start_price_series, sold]\
Baseline accuracy: 
\f1\fs28 \cf2 \cb16 \expnd0\expndtw0\kerning0
0.793\

\f2\fs24 \cf0 \cb1 \kerning1\expnd0\expndtw0 Predicted Accuracy: 
\f1\fs28 \cf2 \cb16 \expnd0\expndtw0\kerning0
0.871\

\f2\fs24 \cf0 \cb1 \kerning1\expnd0\expndtw0 \
\
4/7:\
\
Directions to move forward in:\
\
\
\
- Used regression model to predict end prices for auction listings (x)\
\
- What model is best to use for this kind of regression, where we have NLP involved?	- SVM, \'85?\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0 \cf0 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf0 4/9:\
-\cb17  
\b Does lowering the start price increase the probability of selling an item?
\b0 \cb1 To explore this question, we need to compare the start price and sold_state of items that are similar. So we want to compare items that are as similar as possible, with the only difference being the start price. Then we can make a logistic regression model using just startPrice and check the coefficient to see if if there\'92s a significant correlation. So the problem is to find items that are similar to each other. How do we do this? 
\b Use NLP to vectorize titles, then calculate similarity between items (using title, condition as features), then take one set of similar items, and see if there is a relationship between startPrice and sold_state. \

\b0 Plot Sold (x-axis) vs. Average Start Price \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \cb18 - Using TF-IDF and cosine similarity, we were able to make this plot using a particular listing (the  \'91Auction\'92 listing having the highest number of listings similar to it - which has an index of 3080), and using a similarity threshold of 0.5, and found the mean startPrice for sold items to be about $45, and for unsold items $125. \cb1 \
Let\'92s move on to plotting a change startPrice vs Probability of selling.  \
\
\cb17 - 
\b We want to test if changing the startPrice actually causes my model to increase the likelihood of selling
\b0 .\cb1  So, using my current classifier, I\'92ll take a listing that is very common (has many listings similar to itself), and have the model repeatedly calculate probability of SOLD for a listing, as I increase it\'92s start Price from 0x to 2x. Make a plot of the results. 1\
\cb18 - I\'92m only able to plot 1 plot at a time, so I can\'92t get an aggregate view.  Based on briefly looking at some of the items, most of them seem to follow the pattern whereby as you increase startPrice, the probability of selling decreases. \cb1 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf4 \cb11 - Another exciting problem came up. \cf0 \cb1 \
- Discovered that there are very few auctions with an endPrice between ~120 to ~180, and I\'92m not sure why this is. \
	- \cf4 We should collect more \'91Auction\'92 data with a 120 < endprice < 180 to make up for this shortage (x)\cf0 \
	- use 
\f4\fs26 \cf15 \cb16 \expnd0\expndtw0\kerning0
MinPrice in item filter 
\f2 with 120 to 180 dollars.  (x)\
	- then delete duplicates (x)
\fs24 \cf0 \cb1 \kerning1\expnd0\expndtw0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf4 We have a huge dearth of items with endPrices around $120 to $180. I explored that and then then found that the create_end_listing_type_and_start_price was creating a lot of endListingType values of \'91Invalid URL\'92, which I was then filtering out, which was also filtering out a lot of endPrices in the range 120-180. I thought this meant that the URLs were actually 
\i \cf4 invalid
\i0 \cf4 , but then I discovered that these URLs were actually valid (by using manual HTTP requests instead of the python port). So i looked into how I create the endListingType of \'91Invalid URL\'92 and found that the important condition is to have a startPrice of NaN, so basically a lot of items with endPrice in the range 120-180 had NaN startPrices\
\
So I had a lot of startPrice of NaN, which is weird because my scraper should have collected a startPrice if there is a valid URL. But my clue was that this weird behavior was only occurring for items with a specific range of endPrices, so that led me to believe that the most likely explanation was that I simply had accidentally skipped scraping some items when I set my url_start_index. So to attempt to scrape the right ones, I changed the scraper to query the database for items starting at the LEAST recent, and because I started scraping at $20 and moved up in price over time, I found out that the $120 items begin at around index 39500 in my database. So I started scraping at 39500, and we get to around $180 at index 54000, and scraping this many should take around 4 hours. \
\
Let\'92s see if this works. \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf4 \cb18 Yep, that was the problem. Just needed to scrape every itemId in our database. We\'92re still at a dearth for items with an endPrice > $650, so we need to keep scraping. \cf0 \cb1 \
\
\
4/10:\
Goals:\
- Feature Engineer median startPrice of similar, completed listings\
- Make Pickles \
\
Journal:\
1. Create a pickle out of cleaned auctions DataFrame.\
2. Create a new notebook for feature engineering the median start price of concurrent listings\
	- Create a pile of feature engineered auctions data frame\
	- Import new auctions into old notebook \
\
4/11:\
- Working on feature engineer, but the function is estimated to take around 2 hours. So I\'92ll run it during passover. \
	- It ran in like 10 minutes. Weird. \
\
4/12: \
- Add concurrent similar median start price feature and see if it improves model. \
	- It did not. (x)\
- save EC2 Image (x)\
- Grid Search on EC2 instance for best classifier \
- Find median endPrice of similar, completed listings\
- profitability metric \
	- Take a random sample of data\
	- For each sample:\
		- Calculate sold_probability and predicted_end_price for a range of start prices from 0 to 2x start price. \
		- where sold_probability > 0.5, calculate optimal start price, and optimal end price. sold_state_pred = 1.\
		- If there is no sold_probability > 0.5, then optimal start price = 0 and optimal end price = 0. sold_state_pred = 0.\
		- if  sold_state_pred == 1 and sold_state_true == 1:\
			- Calculate profit_diff = end_price_pred - end_price_true\
		- if sold_state_pred == 1 and sold_state_true == 0:\
			- Calculate profit_diff = end_price_pred - end_price_true (which is 0)\
		- if sold_state_pred == 0 and sold_state_true == 1:\
			- Calculate profit_diff = end_price_pred (which is 0) - end_price_true\
	average_profit_lift = np.mean(profit_diff)\
\
4/13:\
- Created new EC2 instance (x)\
	- Used default Linux AMI ->\
	- Installed Anaconda with:\
		- $ wget https://repo.continuum.io/archive/Anaconda2-4.3.1-Linux-x86_64.sh\
		- $ bash Anaconda2-4.3.1-Linux-x86_64.sh  n\
		- Closed SSH opened new SSH\
		- Transfer files to instance \
		- ssh -i "ebay-price-predictor-3.pem" -L 8000:localhost:8888 ec2-user@ec2-54-183-29-45.us-west-1.compute.amazonaws.com\
\
- (on ec2) Grid Search RandomForestClassifier (X)\
	- run a process in background: nohup python classification-grid-search.py &\
	- $ top -> k -> (PID_#) -> 15\
\
	
\b Best Estimator
\b0 : RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\
             max_depth=10, max_features='auto', max_leaf_nodes=None,\
             min_impurity_split=1e-07, min_samples_leaf=5,\
             min_samples_split=6, min_weight_fraction_leaf=0.0,\
             n_estimators=500, n_jobs=-1, oob_score=False,\
             random_state=None, verbose=0, warm_start=False)\
	- 
\b Best Score
\b0  (roc_auc)  0.6544\
	- 
\b Best Parameters
\b0 : \{'max_features': 'auto', 'min_samples_split': 6, 'n_estimators': 500, 'max_depth': 10, 'min_samples_leaf': 5\}\
\
- (on ec2) Tried ExtraTreesClassifier(n_estimators=25, boostrap=True), which took WAY longer, and didn\'92t improve results. \
\
- Explore distribution of end prices \
	- It\'92s skewed to the left \
	- Should transform endPrice (log(endPrice) = output) and predict that then transform back (endPrice = 10^output ? )\
\
- Create time of day feature  (x)\
	- did not really help \
\
Question to ask vrushank:\
	- How to transform endPrice?\
	- Explain time of day feature didn\'92t help\
	- \
\
\
Presentation:\
\pard\pardeftab720\sl320\partightenfactor0

\f1\fs28 \cf2 \cb16 \expnd0\expndtw0\kerning0
Baseline Accuracy: 0.857667278492\
Baseline RFC accuracy: 0.89738027699\
\
baseline mean_absolute_error: 66.7116406965 \
predicted mean_absolute_error: 27.0460162035\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2\fs24 \cf0 \cb1 \kerning1\expnd0\expndtw0 \
\pard\pardeftab720\sl320\partightenfactor0

\f1\fs28 \cf2 \cb16 \expnd0\expndtw0\kerning0
Optimal Predicted End Price:$618.714156395, Optimal Start Price:$590.0, Chance of Selling:0.87, Expected Profit:$538.281316064\
\pard\pardeftab720\sl320\partightenfactor0

\f2\fs24 \cf0 \cb1 \kerning1\expnd0\expndtw0 \
\pard\pardeftab720\sl320\partightenfactor0

\f1\fs28 \cf2 \cb16 \expnd0\expndtw0\kerning0
Average End Price: $262.72
\f2\fs24 \cf0 \cb1 \kerning1\expnd0\expndtw0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1\fs28 \cf0 Average Lift: $34.20\
% Average Lift: 13% Increased Profit on Average!\
on 20,000 Auction Listings: $\cf2 \cb16 \expnd0\expndtw0\kerning0
6,719,374.15 Net Increased Profit\
\cf0 \cb1 \kerning1\expnd0\expndtw0 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2\fs24 \cf0 \
\
\
Goals:\cf10 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b \cf0 Feature Engineering:
\b0 \
- median startPrice of similar, completed listings (x)\
	- Use preprocessing.normalize() on data and then create SCM start price  feature again\
	- Talk with Vrushank about investigation into SCM feature \
		- do i need to normalize output end Prices since they are not normally distributed?\
		- How to combine valida`tion metric and profitability metric? \
- median endPrice of similar, completed listings \
- time of day listing went on (x)\
\

\b Model Optimization (to combat over-fitting):\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b0 \cf0 - plot a learning curve to figure out how much data I really need until training error & test error converge\
	- Use this information to downsample (using pandas sample) when grid searching (to make models quicker to train) \
- grid search vectorizer settings\
- grid search models \
- grid search model parameters \
- optimize classification models for Recall \
- optimize regression models for mean_absolute_error \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b \cf0 Profitability Optimization Metric:
\b0 \
- Calculate the average expected price increase we were able to get for all items. \
	- Use AWS/Dominoe to do this \
\

\b Identify Listing factors that contribute to changes in probability of selling & predicted end price:
\b0 \
- Recommend replacements/deletion for words that decrease probability\
\

\b Using Images + NN To Predict Sale
\b0 \
\
\
4/14:\
- Extract Listing Features - Model, Lens, MegaPixels\
	- we can use the getItem Call in the Trading API to get category specific information (like Model, MegaPixels)\
	- We used getCategorySpecifics Call in the Trading API to get the \'93Item Specifics\'94 for the Digital Camera (#31388) category. They are:\
Type\
Brand\
MPN\
Series\
Model\
Megapixels\
Optical Zoom\
Features\
Color\
Bundled Items\
Connectivity\
Battery Type\
Manufacturer Warranty\
Screen Size\
Digital Zoom\
Country/Region of Manufacture\
\
4/15:\
\
3 ways to extract Brand, Model information:\
1. Use ebay API. Problematic because we only get a certain number of calls per day.\
2. Scrapy. Long development + takes many hours. \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \cb17 3. Extract Brand,Model from title using Brand,Model information already in category_specific table. Fastest. \cb1 \
\
\
I think the most important features for each listings are:\
Brand - \cb17 Extract Brand,Model from title using Brand,Model information already in category_specific table. Fastest. \cb1 \
Model - \cb17 Extract Brand,Model from title using Brand,Model information already in category_specific table. Fastest. \cb1 \
Lens Type - \cb17 Extract Lens Type from title using string matching (\'93Body\'94=0, \\d-\\d=1, \'93Len\'94=1, \'93mm\'94=1. \cb1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 Condition - Already have (x)\
Seller Feedback - Already have (x)\
Free Shipping - Already have (x)\
Bundled Items / Extas - \cb14 Will need to use scrapy or several days of ebay API. Not likely to happen. \cb1 \
Original Packaging - \cb19 Would need to use scrapy. Not likely to happen\cb1 \
\
Features to Engineer with these features:\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b \cf0 - Median listing price of FixedPrice listings of same Brand,Model, Condition for Classification.\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 - Median end price of Auctions,FixedPrice listings of same Brand,Model, Condition for Regression.
\b0 \
\
\
Get a list of brands/models with:\
select DISTINCT "Brand" from category_specifics;\
select DISTINCT "Model" from category_specifics;\
\
4/16:\
Model Extraction worked really well!!!\
First I extract obvious models with regex (for example 
\f5\fs28 \cf2 \cb16 \expnd0\expndtw0\kerning0
dsc-wx300
\f2 ).
\fs24  Then i use TF-IDF vectorization + cosine similarity to match a filtered version of the title with the list of models pulled using ebay API. From a spot check, it\'92s working really well! More issues with the regex extraction than with TF-IDF actually, so we\'92re going to try to use TF-IDF the whole way. 
\b Actually just did a test - Regex is messing things up, so let\'92s just use TF-IDF + Cosine Similarity to extract model name on all listings. 
\b0 \
\
We also need to make more API calls with getItem so that we have a bigger camera Model list. Right now we only have like 1000 distinct models. Which, with my spot check, has been fine, but we can use some of my 5000 limit to potentially get more models. We could also use the API to get Bundled Items. However I think I want to focus on just auctions at this point. \
So I need to 
\b create a table called 
\f3 category_specifics_auctions
\f2\b0 :\
1. Copy itemId from completed_items_v2 where listingType != \'91FixedPrice\'92 and listingType != \'91StoreInventory\'92 into table called category_specifics_auctions\
2. Update category_specifics_auctions using rows from category_specifics that have non-empty Brands.\
TOO TIME-INTENSIVE!\
We\'92re just going to use category_specifics to get a list of brands and models. \
We will need to use scrapy if we want to get Bundled Items.\
\
Lens extraction with regex went fine, but I realized that the 18-55mm text is only PART of the lens description. There 
\f1\fs28 is more to the lens than the 15-55mm
\f2\fs24 \cf0 \cb1 \kerning1\expnd0\expndtw0 \
\
Okay so Model Extraction did NOT work really well. I don\'92t know why I thought it worked well earlier. \
I still need to extract brand, model information from title. But I need a CLEAN LIST of models to compare with the listing title. So In order to get a clean list, I\'92ll scrape B&H Video for all their cameras, and store that information into a table in my database called b_h_camera_inventory\
Create table:\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f1 \cf0 CREATE TABLE b_h_digital_camera_inventory (\
	"id" 				SERIAL PRIMARY KEY,\
	"Brand" 			TEXT,\
	"Model" 			TEXT,\
	"Retail Price" 		DECIMAL,\
	"Body Only" 		BOOLEAN,\
	"Kit"				BOOLEAN,\
	"Has Lens" 			BOOLEAN,\
	"Lens"				TEXT,\
	"B&H Id" 			TEXT,\
	"Title"			TEXT\
);
\f2 \
\
# -*- coding: utf-8 -*-\
\
# Define here the models for your scraped items\
#\
# See documentation in:\
# http://doc.scrapy.org/en/latest/topics/items.html\
\
import scrapy\
\
\
class CameraRetailerScraperItem(scrapy.Item):\
    # define the fields for your item here like:\
    # name = scrapy.Field()\
    brand = scrapy.Field(default='NULL')\
    model = scrapy.Field(default='NULL')\
    retailPrice = scrapy.Field(default='NULL')\
    bodyOnly = scrapy.Field(default='NULL')\
    kit = scrapy.Field(default='NULL')\
    hasLens = scrapy.Field(default='NULL')\
    lens = scrapy.Field(default='NULL')\
    bhId = scrapy.Field(default='NULL')\
\
\
\
4/17:\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b\fs32 \cf0 Classification
\b0\fs24 \

\b\fs28 pd.read_pickle('./pickles/df_classification_count_vec.p')
\b0\fs24  \
\
RandomForestClassifier(n_estimators=100, n_jobs=-1, verbose=1) \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \ul \ulc0 Not Cross Val\ulnone \
Baseline Accuracy: 0.847\
Model accuracy: 0.903\
\ul 3-Fold Cross Val:\ulnone \
\
Logistic Regression:\
\ul Not\ulnone  \ul Cross Val:\ulnone \
Baseline Accuracy: 0.8475 \
Model accuracy: 0.8754\
\ul 3-Fold\ulnone  \ul Cross Val:\ulnone \
\cb20 Accuracy: 0.781099028892\cb1 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 Logistic Regression Interesting Important Features:\
6th (u\'92fast', 1.87)\
9th (u\'92box', 1.6658)\
18th (u\'92gently used', 1.3718)\
25th (u\'92good cosmetic', 1.0716)\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
Classification Ensemble (RF, LR, XG):\
\ul KFold Cross Val:\ulnone \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 [(0.94599636950383215, 0.89573459715639814),  \
(0.94433519891090612, 0.88674868898749493),  \
(0.94418393586446836, 0.89179104477611937)]\
\cb21 Overfitting.\cb1 \
Baseline Accuracy: 0.854 
\f1 \
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2 \cf0 \cb22 Cross Validated Ensemble GMean Prediction Accuracy: 0.891\
Increase Accuracy due to model: 0.036\cb1 \
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b\fs32 \cf0 Regression\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\b0\fs24 \cf0 RandomForestRegressor\
\pard\pardeftab720\sl320\partightenfactor0

\f1\fs28 \cf23 \cb24 \expnd0\expndtw0\kerning0
\outl0\strokewidth0 \strokec23 Average Cross Validated RFR Score: -41.1394388889\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f2\fs24 \cf0 \cb1 \kerning1\expnd0\expndtw0 \outl0\strokewidth0 \
\
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0
\cf0 \
\
\
\
\
\
\
}